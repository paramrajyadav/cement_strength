# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11nJG2oev4E3ELM5UNv5qqd4IRxhZJGJq
"""

!pip install pyspark
!pip install pyspark
!pip install findspark

from google.colab import drive
drive.mount('/content/drive')



import findspark
findspark.init()

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("HistogramExample").config("spark.ui.enabled", "false").getOrCreate()

df = spark.read.csv('/content/drive/MyDrive/Dataset/Training_File.csv', header=True, inferSchema=True)

columns = df.columns

# Select all columns except the last one for DataFrame X
x_columns = columns[:-1]
df_x = df.select(x_columns)

# Select the last column for DataFrame Y
y_column = columns[-1]
df_y = df.select(y_column)

# Show the DataFrames

df.show()
df.printSchema()
df.describe().show()

from functools import reduce

from pyspark.sql.functions import isnan, isnull
columns = ['Cement', 'Age_day', 'Water_component_4', 'Fly Ash _component_3', 'Fine Aggregate_component_7','Age_day','Concrete_compressive _strength']
df_na = df.filter(reduce(lambda x, y: x | y, (isnan(c) | isnull(c) for c in columns)))
df_na.show()

df.columns

import pandas as pd
import numpy as np

from pyspark.sql import SparkSession
import matplotlib.pyplot as plt

# Create Spark session
#spark = SparkSession.builder.appName("HistogramExample").config("spark.ui.enabled", "false").getOrCreate()


# Load data
#data = spark.read.csv("data.csv", header=True, inferSchema=True)

# Get column names (excluding non-numeric columns)
numeric_columns = [col for col, dtype in df.dtypes if dtype in ['int', 'double','float']]

# Loop through numeric columns and plot histograms
for column in numeric_columns:
    column_data = df.select(column)
    pandas_df = column_data.toPandas()

    plt.hist(pandas_df[column], bins=20, color='blue', alpha=0.7)
    plt.xlabel("Values")
    plt.ylabel("Frequency")
    plt.title(f"Histogram of {column}")
    plt.show()

# Stop Spark session
#spark.stop()

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, log
import numpy as np

# Create Spark session
#spark = SparkSession.builder.appName("HistogramExample").config("spark.ui.enabled", "false").getOrCreate()

# Load data into DataFrame (replace "data.csv" with your actual file)
#ds1 = spark.read.csv("data.csv", header=True, inferSchema=True)

# Clone the DataFrame ds1 into ds22
ds22 = df

# Perform the operations on each column
for column in df.columns:
    df = df.withColumn(column, col(column) + 1)  # Adding 1 to each column
    df = df.withColumn(column, log(col(column)))  # Taking the natural logarithm

# Show the transformed DataFrame
df.show()

# Stop Spark session
#spark.stop()

from pyspark.sql import SparkSession
import matplotlib.pyplot as plt

# Create Spark session
#spark = SparkSession.builder.appName("HistogramExample").config("spark.ui.enabled", "false").getOrCreate()


# Load data
#data = spark.read.csv("data.csv", header=True, inferSchema=True)

# Get column names (excluding non-numeric columns)
numeric_columns = [col for col, dtype in df.dtypes if dtype in ['int', 'double','float']]

# Loop through numeric columns and plot histograms
for column in numeric_columns:
    column_data = df.select(column)
    pandas_df = column_data.toPandas()

    plt.hist(pandas_df[column], bins=20, color='blue', alpha=0.7)
    plt.xlabel("Values")
    plt.ylabel("Frequency")
    plt.title(f"Histogram of {column}")
    plt.show()

# Stop Spark session
#spark.stop()

pandas_df_box_plot = df.toPandas()

# Generate box plots for each column
for column in pandas_df_box_plot.columns:
    plt.figure(figsize=(6, 4))
    plt.boxplot(pandas_df_box_plot[column], vert=False)
    plt.xlabel(column)
    plt.title(f"Box Plot of {column}")
    plt.show()

from pyspark.sql import SparkSession
import matplotlib.pyplot as plt

# Create Spark session
#spark = SparkSession.builder.appName("DiscreteGraphsExample").getOrCreate()

# Load data into DataFrame (replace "data.csv" with your actual file)
#ds1 = spark.read.csv("data.csv", header=True, inferSchema=True)

# Clone the DataFrame ds1 into ds22
#ds22 = ds1

# Convert Spark DataFrame to Pandas DataFrame
pandas_df_scatter_plot= df.toPandas()

# Extract the last column as y-axis data
y_axis_column = pandas_df_scatter_plot.columns[-1]
y_data = pandas_df_scatter_plot[y_axis_column]

# Create discrete graphs (bar plots) using Matplotlib
for column in pandas_df_scatter_plot.columns[:-1]:
    plt.figure(figsize=(8, 6))
    plt.scatter(pandas_df_scatter_plot[column], y_data)
    plt.xlabel(column)
    plt.ylabel(y_axis_column)
    plt.title(f"Scatter Plot: {column} vs {y_axis_column}")
    plt.show()

# Stop Spark session
#spark.stop()

from pyspark.ml.feature import VectorAssembler

# Define the columns we want to use
input_cols = df.columns

# Create the VectorAssembler
vec_assembler = VectorAssembler(inputCols=input_cols, outputCol="features")

# Transform the data
df_heat = vec_assembler.transform(df)

from pyspark.ml.stat import Correlation

# Compute the correlation matrix
corr_matrix = Correlation.corr(df_heat, "features").head()

# Convert the correlation matrix to a DataFrame
corr_df = spark.createDataFrame(corr_matrix[0].toArray())

import seaborn as sns
import matplotlib.pyplot as plt

# Convert the Spark DataFrame to a Pandas DataFrame
pandas_df_heat = corr_df.toPandas()

# Create the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(pandas_df_heat, annot=True, cmap='coolwarm')

# Show the plot
plt.show()

df.show()

df_x.show()
df_y.show()

from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import KMeans
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml.evaluation import ClusteringEvaluator

# Create a Spark session
#spark = SparkSession.builder \
#    .appName("ClusteringHyperparamTuning") \
#    .getOrCreate()

# Load and prepare data
#data = spark.read.csv("data.csv", header=True, inferSchema=True)
feature_columns = ['Cement',
 'Blast Furnace Slag _component_2',
 'Fly Ash _component_3',
 'Water_component_4',
 'Superplasticizer_component_5',
 'Coarse Aggregate_component_6',
 'Fine Aggregate_component_7',
 'Age_day']
assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")
data_with_features = assembler.transform(df)
kmeans = KMeans()
# Define the hyperparameter grid
param_grid = ParamGridBuilder() \
    .addGrid(kmeans.k, [2, 3, 4, 5,6,7,8,9,10,11]) \
    .addGrid(kmeans.initMode, ['k-means||', 'random']) \
    .addGrid(kmeans.maxIter, [10, 20, 30]) \
    .build()

# Define the evaluator
evaluator = ClusteringEvaluator()

# Create a K-means instance
kmeans = KMeans(featuresCol='features', predictionCol='prediction')

# Create a CrossValidator instance
crossval = CrossValidator(estimator=kmeans,
                          estimatorParamMaps=param_grid,
                          evaluator=evaluator,
                          numFolds=5)  # Number of folds in cross-validation

# Perform cross-validation and get the best model
cv_model = crossval.fit(data_with_features)
best_kmeans_model = cv_model.bestModel

# Get the best hyperparameters
best_k = best_kmeans_model.getK()
best_init_mode = best_kmeans_model.getInitMode()
best_max_iter = best_kmeans_model.getMaxIter()
print(f"Best K: {best_k}, Best Init Mode: {best_init_mode}, Best Max Iter: {best_max_iter}")

# Assign clusters to data
clustered_data = best_kmeans_model.transform(data_with_features)

# Evaluate clustering results
silhouette_score = evaluator.evaluate(clustered_data)
print(f"Silhouette Score: {silhouette_score}")

# Stop the Spark session
#spark.stop()

ds1=df_x
ds2=df_y
ds3=data_with_features
ds4=clustered_data

ds4.show()

unique = clustered_data.select("prediction").distinct().rdd.flatMap(lambda x: x).collect()



modal={}
for i in unique:
  from pyspark.sql import SparkSession
  from pyspark.ml.feature import VectorAssembler
  from pyspark.ml.regression import GBTRegressor
  from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit
  from pyspark.ml.evaluation import RegressionEvaluator

  # Create a Spark session
  #spark = SparkSession.builder \
  #    .appName("XGBoostHyperparamTuning") \
  #    .getOrCreate()

  # Load data from a CSV file
  #data_file_path = "path/to/your/data.csv"  # Replace with the actual path
  #df = spark.read.csv(data_file_path, header=True, inferSchema=True)
  df_1 = clustered_data.filter(clustered_data.prediction == i)
  new_df = df_1.drop("prediction","features")

  # Assuming you have a "features" column and a "target" column
  feature_columns = ['Cement',
  'Blast Furnace Slag _component_2',
  'Fly Ash _component_3',
  'Water_component_4',
  'Superplasticizer_component_5',
  'Coarse Aggregate_component_6',
  'Fine Aggregate_component_7',
  'Age_day']  # Replace with your feature column names
  assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")
  df_1 = assembler.transform(new_df)

  #df_1.show()

# Define the hyperparameter grid for XGBoost
  param_grid = ParamGridBuilder() \
    .addGrid(GBTRegressor.maxDepth, [5, 10, 15]) \
    .addGrid(GBTRegressor.maxBins, [20, 30, 40]) \
    .build()

# Define the evaluator for regression tasks
  evaluator = RegressionEvaluator(metricName="rmse", labelCol="Concrete_compressive _strength")

# Create a GBTRegressor (Gradient-Boosted Trees) instance
  gbt_regressor = GBTRegressor(featuresCol="features", labelCol="Concrete_compressive _strength")

# Create a TrainValidationSplit instance
  tvs = TrainValidationSplit(estimator=gbt_regressor,
                          estimatorParamMaps=param_grid,
                          evaluator=evaluator,
                          trainRatio=0.8)  # Train-validation split ratio

# Fit the TrainValidationSplit model
  tvs_model = tvs.fit(df_1)

# Get the best model from the TrainValidationSplit
  best_gbt_model = tvs_model.bestModel

  modal[i]=best_gbt_model
# Optionally, you can extract the best hyperparameters from the best model
  best_max_depth = best_gbt_model.getMaxDepth()
  best_max_bins = best_gbt_model.getMaxBins()

# Print the best hyperparameters

unique = clustered_data.select("prediction").distinct().rdd.flatMap(lambda x: x).collect()

modal

from pyspark.sql import SparkSession
from pyspark.sql import Row

# Create a Spark session
spark = SparkSession.builder \
    .appName("CreateDataFrameExample") \
    .getOrCreate()

# Sample data for a single row
data = [
    (540.0, 0.0, 0.0, 162.0, 2.5, 1040.0, 676.0, 28)
]

# Define the columns
columns = [
    "Cement",
    "Blast Furnace Slag _component_2",
    "Fly Ash _component_3",
    "Water_component_4",
    "Superplasticizer_component_5",
    "Coarse Aggregate_component_6",
    "Fine Aggregate_component_7",
    "Age_day"
]

# Create a Row object using the sample data
row = Row(*data[0])

# Create a DataFrame with a single row
df = spark.createDataFrame([row], columns)

# Show the DataFrame
df.show()

# Stop the Spark session
#spark.stop()

columns=['Cement',
 'Blast Furnace Slag _component_2',
 'Fly Ash _component_3',
 'Water_component_4',
 'Superplasticizer_component_5',
 'Coarse Aggregate_component_6',
 'Fine Aggregate_component_7',
 'Age_day']
assembler = VectorAssembler(inputCols=columns, outputCol="features")
df = assembler.transform(df)

#new_df = assembler.transform(df)
predictions = best_kmeans_model.transform(df)

# Show the predictions
predictions.show()

# Stop the Spark session
#spark.stop()

predictions

c1 = predictions.select("prediction").distinct()
df_2 = predictions.drop("prediction")

unique_ids = predictions.select("prediction").distinct().rdd.flatMap(lambda x: x).collect()
unique_ids = [int(id) for id in unique_ids]

x=int(unique_ids[0])

type(x)

p1=modal[x].transform(df_2)

p1.show()

